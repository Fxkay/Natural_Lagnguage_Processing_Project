{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119ca070",
   "metadata": {},
   "source": [
    "Natural Lagnguage Processing Challenge\n",
    "================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66442017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score , mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40b6c1",
   "metadata": {},
   "source": [
    "Checking and Understanding Datesets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf25919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ordinary dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88035718",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the info of the dataset\n",
    "\n",
    "data.info(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8402c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing NAN\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb6b64",
   "metadata": {},
   "source": [
    "The Section Processing the Text for Machine Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a923f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51080e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries for this project\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b11c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training data\n",
    "#data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializinig lemmatizer and stop words with fallback\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english')) if _have_stopwords else set()\n",
    "except Exception:\n",
    "    stop_words = set()\n",
    "# this code initializes a lemmatizer and a set of stop words,\n",
    "# with a fallback to an empty set if there's an issue loading the stop words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb485233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The Pro- reprocessing pf function I wast remove the symbols but keep the numbers\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Removes symbols but keep numbers\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    # Removes stopwords & lemmatize\n",
    "    tokens = [w for w in text.split() if w not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb63853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making  sure the required columns exist\n",
    "required_cols = {'label','title','text'}\n",
    "missing = required_cols - set(data.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in data.csv: {missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3d0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pre-processing ONLY to title and text, as requested\n",
    "cols_to_clean = ['title', 'text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc8f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  apply per column\n",
    "for c in cols_to_clean:\n",
    "    data[c] = data[c].fillna(\"\").astype(str).apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating feature column: title + text\n",
    "data['clean_text'] = (data['title'] + \" \" + data['text']).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed0e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Train/test split\n",
    "X = data['clean_text']\n",
    "y = data['label'].astype(int)  # ensure int labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dea070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))  # unigrams+bigrams often help\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec  = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd88cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 8: Train classifier\n",
    "model = LogisticRegression(max_iter=2000, n_jobs=None)  # bump max_iter for convergence\n",
    "model.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5dca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 9: Evaluate\n",
    "y_pred = model.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, digits=3)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd99ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "print(\"Accuracy:\", round(accuracy, 4))\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "\n",
    "dump(model, \"logisticregresionl.joblib\")\n",
    "dump(vectorizer, \"tfidf_vectorizer.joblib\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39085b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SGDClassifier\n",
    "# loss=\"log_loss\" → logistic regression\n",
    "# loss=\"hinge\" → linear SVM\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model = SGDClassifier(\n",
    "    loss=\"log_loss\",       # logistic regression\n",
    "    penalty=\"l2\",          # regularization\n",
    "    max_iter=1000,\n",
    "    tol=1e-3,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "y_pred = model.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, digits=3)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", round(accuracy, 4))\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "\n",
    "# Save model & vectorizer\n",
    "\n",
    "dump(model, \"sgd_classifier_model.joblib\")\n",
    "dump(vectorizer, \"tfidf_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using  (SVM)\n",
    "\n",
    "\n",
    "# This is for saving  models for later\n",
    "from joblib import dump\n",
    "\n",
    "\n",
    "# Ensuring am only using the required columns i want title and text for context\n",
    "required_cols = {\"label\", \"title\", \"text\"}\n",
    "missing = required_cols - set(data.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in data.csv: {missing}\")\n",
    "\n",
    "# I cleaned only title and text becasue I only focus of context\n",
    "for c in [\"title\", \"text\"]:\n",
    "    data[c] = data[c].fillna(\"\").astype(str).apply(preprocess_text)\n",
    "\n",
    "# Combineing the cleaned into one feature column\n",
    "data[\"clean_text\"] = (data[\"title\"] + \" \" + data[\"text\"]).str.strip()\n",
    "\n",
    "\n",
    "# Train Test and split\n",
    "\n",
    "X = data[\"clean_text\"]\n",
    "y = data[\"label\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec  = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# Training using  another classifier LinearSVC (SVM)\n",
    "\n",
    "model = LinearSVC()  # the model\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "\n",
    "# Evaluating the meodle\n",
    "\n",
    "y_pred = model.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, digits=3)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", round(accuracy, 4))\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "\n",
    "# finally saving the model \n",
    "\n",
    "dump(model, \"svm_model.joblib\")\n",
    "dump(vectorizer, \"tfidf_vectorizer.joblib\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096462a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from joblib import load\n",
    "\n",
    "# Load saved model and vectorizer\n",
    "model = load(\"svm_model.joblib\")\n",
    "vectorizer = load(\"tfidf_vectorizer.joblib\")\n",
    "\n",
    "\n",
    "# Load validation data\n",
    "val_data = pd.read_csv(\"validation_data.csv\")\n",
    "\n",
    "# Preprocess title and text columns\n",
    "for col in [\"title\", \"text\"]:\n",
    "    val_data[col] = val_data[col].fillna(\"\").astype(str).apply(preprocess_text)\n",
    "\n",
    "# Combine into one input feature\n",
    "val_data[\"clean_text\"] = (val_data[\"title\"] + \" \" + val_data[\"text\"]).str.strip()\n",
    "\n",
    "# Vectorize using loaded vectorizer\n",
    "X_val_vec = vectorizer.transform(val_data[\"clean_text\"])\n",
    "\n",
    "# Predict labels (0 or 1)\n",
    "predictions = model.predict(X_val_vec)\n",
    "\n",
    "# Replace label column with predicted labels\n",
    "val_data[\"label\"] = predictions\n",
    "\n",
    "# Ensure required output format\n",
    "expected_cols = [\"label\", \"title\", \"text\", \"subject\", \"date\"]\n",
    "if not set(expected_cols).issubset(val_data.columns):\n",
    "    raise ValueError(f\"Missing expected columns: {set(expected_cols) - set(val_data.columns)}\")\n",
    "\n",
    "# Save output with original format\n",
    "val_data[expected_cols].to_csv(\"predicted_validation_data.csv\", index=False)\n",
    "print(\"✅ Predictions saved to predicted_validation_data_NEW.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b936ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Natural Lagnguage Processing Challenge (Complement Naive Bayes)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Ensure required columns\n",
    "required_cols = {\"label\", \"title\", \"text\"}\n",
    "missing = required_cols - set(data.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in data.csv: {missing}\")\n",
    "\n",
    "# Clean title and text\n",
    "for c in [\"title\", \"text\"]:\n",
    "    data[c] = data[c].fillna(\"\").astype(str).apply(preprocess_text)\n",
    "\n",
    "# Combine into one column\n",
    "data[\"clean_text\"] = (data[\"title\"] + \" \" + data[\"text\"]).str.strip()\n",
    "\n",
    "\n",
    "# Train/Test split\n",
    "\n",
    "X = data[\"clean_text\"]\n",
    "y = data[\"label\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# Train with ComplementNB\n",
    "\n",
    "model = ComplementNB()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "y_pred = model.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, digits=3)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", round(accuracy, 4))\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "\n",
    "# Save model & vectorizer\n",
    "\n",
    "dump(model, \"complement_nb_model.joblib\")\n",
    "dump(vectorizer, \"tfidf_vectorizer.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90086cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  (Random Forest)\n",
    "\n",
    "\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_cols = {\"label\", \"title\", \"text\"}\n",
    "missing = required_cols - set(data.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in data.csv: {missing}\")\n",
    "\n",
    "# Clean title and text\n",
    "for col in [\"title\", \"text\"]:\n",
    "    data[col] = data[col].fillna(\"\").astype(str).apply(preprocess_text)\n",
    "\n",
    "# Combine into one text column\n",
    "data[\"clean_text\"] = (data[\"title\"] + \" \" + data[\"text\"]).str.strip()\n",
    "\n",
    "\n",
    "# Train/Test split\n",
    "\n",
    "X = data[\"clean_text\"]\n",
    "y = data[\"label\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (SGDClassifier)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_cols = {\"label\", \"title\", \"text\"}\n",
    "missing = required_cols - set(data.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in data.csv: {missing}\")\n",
    "\n",
    "# Clean title and text\n",
    "for col in [\"title\", \"text\"]:\n",
    "    data[col] = data[col].fillna(\"\").astype(str).apply(preprocess_text)\n",
    "\n",
    "# Combine into one text column\n",
    "data[\"clean_text\"] = (data[\"title\"] + \" \" + data[\"text\"]).str.strip()\n",
    "\n",
    "\n",
    "# Train/Test split\n",
    "\n",
    "X = data[\"clean_text\"]\n",
    "y = data[\"label\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# Train SGDClassifier\n",
    "# loss=\"log_loss\" → logistic regression\n",
    "# loss=\"hinge\" → linear SVM\n",
    "\n",
    "model = SGDClassifier(\n",
    "    loss=\"log_loss\",       # logistic regression\n",
    "    penalty=\"l2\",          # regularization\n",
    "    max_iter=1000,\n",
    "    tol=1e-3,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "y_pred = model.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, digits=3)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", round(accuracy, 4))\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "\n",
    "# Save model & vectorizer\n",
    "\n",
    "dump(model, \"sgd_classifier_model.joblib\")\n",
    "dump(vectorizer, \"tfidf_vectorizer.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model names and their accuracies\n",
    "models = [\"SGDClassifier\", \"Naive Bayes\", \"SVM\", \"Logistic Regression\" ]\n",
    "accuracies = [0.9817, 0.9415, 0.995, 0.9875  ]\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(9, 6))\n",
    "bars = plt.bar(models, accuracies, color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\" ])\n",
    "\n",
    "# Add accuracy values on top of each bar\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
    "             f\"{acc:.4f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Labels and title\n",
    "plt.ylim(0.93, 1.0)  # focus on the high accuracy range\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iron_hack_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
